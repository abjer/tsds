{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **DO NOT EDIT IF INSIDE `tsds` folder** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 6: Networks 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Wednesday, March 13th, 2019*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-23T15:30:03.634114Z",
     "start_time": "2017-08-23T15:30:03.629294Z"
    }
   },
   "source": [
    "Networks are mathematical representations of complex systems. We can use networks to gain various statistical insight about the system we're representing, but we can also use the network as a substrate on which to model phenomena and simulate processes. Some systems may have clusters: we can discover those by employing *community detection* algorithms. Other systems may be perfect for simulating how diseases spread. This week we will explore these two concepts. You will learn:\n",
    "\n",
    "* How a popular community detection algorithm works.\n",
    "* How null models are used in networks.\n",
    "* How to simulate information/epidemic spreading on a network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Questions**: Outside of class, use [issue on GitHub](https://github.com/abjer/tsds/issues) for asking questions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-12T10:56:27.700322Z",
     "start_time": "2019-03-12T10:56:27.695950Z"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pylab as plt\n",
    "import networkx as nx           # `pip install networkx`\n",
    "import json\n",
    "from collections import Counter\n",
    "import community                # `pip install python-louvain` or `conda install -c auto python-louvain`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1: Community detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Communities are little lumps of nodes in a network that are unusually strongly connected. Your family is a community, and your friend group from work or university is another community. While there is no one true definition of what a community is, there are many many different methods and algorithms for finding them. Here we will work with one of the most popular ones: [Louvain Modularity](https://en.wikipedia.org/wiki/Louvain_Modularity). The following exercises will walk you through the fundamentals of this method, and finally have you apply it to the network you used last week."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The modularity function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 6.1.1**: Assume you have a network where nodes and links tend to form lumps here and there. Imagine you now reach for your pen, and start labeling these nodes with group names (or ids) that feel appropriate according to how they are lumped together. If your partition is \"good\", nodes that are connected in groups should intuitively have the same label, while nodes that are distant and disconnected should have different labels. Modularity is a function that can be used to measure *how good* your partition is. It is in technical terms a *utility function*, and it looks like this:\n",
    "> <br><br>\n",
    "> $$ Q = \\dfrac{1}{2m}\\sum_{ij}{\\left[A_{ij}-\\dfrac{k_ik_j}{2m}\\right]\\delta(c_i, c_j)}.$$\n",
    "> <br>\n",
    "> Your job in this problem is to explain this equation. When I look at daunting math I tend to think of it as a program. Since all math can be implemented in code, all math can be broken into parts, where each part has it's own little job to perform. Answer each question below seperately:\n",
    "1. In code, a sum, $\\sum$, is like a `for` loop, where in every iteration you increment a variable. In the equation for modularity the little $ij$ subscript tells is what the sum is looping over (like `for ij in sumloop`). But what is $ij$?\n",
    "2. In each iteration of the sum, the delta function $\\delta(c_i, c_j)$ is used, where $c_i$ is the community label of node $i$. The delta function is a very simple program that returns 0 if the two input values are different and 1 if they are they same. How would you implement the delta function in code? What is it used for in the modularity equation?\n",
    "3. Inside the sum we use the term $\\frac{k_ik_j}{2m}$ as our *null model*. $k$ is the degree sequence (so $k_i$ is the degree of node $i$) and $m$ is the sum of all link weights. Explain what this null model measures. Could we have used other null models?\n",
    "4. The sum subtracts the null model from $A_{ij}$ and adds the result to its final value if the delta function evaluates to 1. What is the point of only summing over this difference when the delta function is 1?\n",
    "5. The sum term is normalized by $2m$. Why exactly $2m$?\n",
    "6. Summarize your insight gained from answering the above questions. In your own words, explain how the modularity function works. Use 1-3 sentences.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 6.1.2**: Implement the modularity function. Write a Python function that takes as input an adjacency matrix and a label vector, and returns the modularity. Compute and print the modularity for the ones given below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-12T10:27:43.909671Z",
     "start_time": "2019-03-12T10:27:43.900257Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "A = np.array([\n",
    "    [0, 1, 1, 0, 0, 0],\n",
    "    [1, 0, 1, 0, 0, 0],\n",
    "    [1, 1, 0, 1, 0, 0],\n",
    "    [0, 0, 1, 0, 1, 1],\n",
    "    [0, 0, 0, 1, 0, 1],\n",
    "    [0, 0, 0, 1, 1, 0],\n",
    "])\n",
    "\n",
    "c = [0, 0, 0, 0, 1, 1]\n",
    "\n",
    "def modularity(A, c):\n",
    "    \"\"\"Compute modularity for a labeled network.\n",
    "    \n",
    "    Input\n",
    "    -----\n",
    "        A : numpy.array\n",
    "            Adjacency matrix. (N, N) square matrix.\n",
    "        c : list of ints\n",
    "            Community labels. Length N.\n",
    "    \n",
    "    Output\n",
    "    ------\n",
    "        out : float\n",
    "    \"\"\"\n",
    "    # Your beautiful code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 6.1.3**: The example labeling, `c`, was not optimal. Find the optimal one and print its modularity score.\n",
    "\n",
    ">*Hint: Either just try a bunch of different label combinations or visualize the networks with node colors that correspond to community, so you can see what is optimal.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finding the best communities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, so we are now able to evaluate the quality of a partition. But how do we find the best partition? Modularity gives us a way to measure *how good* our partition is, but it does't tell of how to find the best one. For that we need some sort of algorithm. The *Louvain method* is that algorithm.\n",
    "\n",
    "It works in the following steps:\n",
    "1. Set every node to be its own community (initiate `c = [0, 1, 2, 3, 4, 5]`).\n",
    "2. Compute the modularity.\n",
    "3. Now pick a random node.\n",
    "    1. For every neighbor it has, try giving it the neighbor's label, and compute the change in modularity.\n",
    "    2. If any of those relabelings led to an increase in modularity, choose the relabeling with the greatest increase.\n",
    "4. Repeat 2-3 until modularity ceases to increase for any relabelings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 6.1.BONUS**: Implement the Louvain method, and show that it gives the labeling for `A`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Communication communities on Facebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's work with some real data. Whip out the network you created **last week**, we will be using that again. Apply again the **threshold** you created in **Ex. 5.2.4**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 6.1.4**: Find the communities in this networks. Print the number of communities and plot the distribution of community sizes. See something interesting? Comment on this distribution.\n",
    "\n",
    ">*Hint: You're welcome to use your own implementation of the Louvain algorithm (pretty badass if you can), but there's also a widely used Python implementation that you can take off the shelf. Go ahead and install `python-louvain` by running `conda install -c auto python-louvain` in a terminal. After installation, import it with `import community`, and use `community.best_partition` to get a node-community dictionary.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 6.1.5**: Visualize the network, similarly to how you did it last week but this time coloring the nodes by their labels.\n",
    ">\n",
    "> *Hint: [Here](https://github.com/benmaier/netwulf#attributes)'s an example of how to use `netwulf` with labels. Also, fiddle around with the layout a little it always makes the network look nicer.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: Contagion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have a network where links represent interactions that can happen between actors, then you can use that as a model universe for simulating the spreading of a disease, a rumor, or some piece of information. There's a whole subfield of Network Science called Contagion Theory which deals with this, but the fundamental idea is actually quite simple:\n",
    "\n",
    "> Nodes that are connected can pass an epidemy/information with some probability and eventually things stabilize.\n",
    "\n",
    "You can then make this more nuanced and step into the realm of *complex contagion* by *inventing* that nodes can only be infected if multiple neighbors are infected, that they can be cured and become resistent/susceptible with some probability, that they may die and exit the simulation creating a void in the network, that becoming infected makes them change their links, that godzilla walks through the door and eats everyoneâ€”the possibilities are endless and you decide which rules exist in the simulation universe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generally speaking, there are three four classes of spreading models:\n",
    "* *Susceptible-Infected (SI)*: Nodes are susceptible at start, and once they are infected they never recover. This models spreading of unforgetable information, or diseases that people get and pass on, and never recover from (like HIV, herpes, etc.).\n",
    "* *Susceptible-Infected-Susceptible (SIS)*: Nodes are susceptible, then get infected and stay infected for some time, then get susceptible again, an the cycle continues. This could model things like the flu which comes and goes and you get and recover from over and over again.\n",
    "* *Susceptible-Infected-Resistant (SIR)*: Nodes become infected and after some time they gain resistance so that they can niether become infected nor spread disease/information further. Once they are resistant they are essentially \"removed\" from the network, so this type of model works for diseases where people recover and gain resistance as well as ones that kill people, as niether type of person can pass whatever is passed around.\n",
    "* *Susceptible-Infected-Resistant-Susceptible (SIRS)*: Probably self-explanatory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 6.2.1**: Computational biologist/physicist and all around cool guy, Dirk Brockmann,\n",
    "makes interactive articles called *Explorables* that introduce ideas in complexity science. Read/play with\n",
    "[his explorable](http://www.complexity-explorables.org/explorables/neighbors/) on targeted vaccination to mitigate epidemic spreading in the real world and answer the\n",
    "following questions:\n",
    "> 1. In the BA network, roughly what percentage of the population do you have to vaccinate with **strategy A** to only get around 10\n",
    "infected individuals? What about **startegy B** and **strategy C**?\n",
    "> 2. **strategy A** and **strategy C** are both entirely random, but one is much more effective than the other. Explain\n",
    "why. What clever idea is **strategy C** employing?\n",
    "> 3. Why is this effect larger in the BA network than in the ER network?\n",
    "> 4. Can you explain why the title mentions Facebook?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 6.2.2**: Choose one of these two explorables ([1](http://www.complexity-explorables.org/explorables/herd/) or [2](http://www.complexity-explorables.org/explorables/epidemonic/)) read/play with and understand it, and explain in your own words what phenomenon it communicates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 6.2.3**: The simplest spreading process is the *SI* model. Below I have implemented such a simulation.\n",
    "1. Comment the code abundantly, explaining what's happing at each step.\n",
    "2. Adapt the code and make a plot of the fraction of infected over time in your facebook network. You may have to restart it a couple of time to make sure patient 0 is someone connected to the rest of the population.\n",
    "3. Extend the code to an *SIS* model, such that after a variable number of timesteps `T_I` infected nodes become susceptible again. Set `T_I = 10` to begin with and plot the fraction of infected over time for 200 timesteps. Does the result make sense? Comment on what you see.\n",
    "4. What is the effect of lowering or increasing `T_I`? What about `p_I`? Intuitively, which qualities of diseases (physical or informational, think rumours for example) do these parameters model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "p_I = 0.1\n",
    "n_iter = 200\n",
    "G = yournetwork\n",
    "\n",
    "I = set()\n",
    "S = set(G.nodes())\n",
    "\n",
    "patient0 = np.random.choice(list(S))\n",
    "\n",
    "I.add(patient0)\n",
    "S.remove(patient0)\n",
    "\n",
    "for t in range(n_iter):\n",
    "    for infected_node in list(I):\n",
    "        neighbors = G.neighbors(infected_node)\n",
    "        infected_neighbors = set([n for n in neighbors if np.random.random() < p_I])\n",
    "        I |= infected_neighbors\n",
    "        S -= infected_neighbors"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  },
  "nav_menu": {},
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
