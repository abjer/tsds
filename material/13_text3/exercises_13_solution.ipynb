{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Before you run solve this make sure you have created a new conda environment.\n",
    "run the following commands in your commandline\n",
    "<pre><code>\n",
    "conda create -n deepmoji anaconda\n",
    "source activate deepmoji\n",
    "## add to jupyter notebook list\n",
    "python -m ipykernel install --user --name deepmoji --display-name \"Python (deepmoji)\"</code></pre>\n",
    "\n",
    "### Or preferably load this notebook into [google colab](https://colab.research.google.com)   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfer learning and bias detection\n",
    "Today we are gonna practice adopting pretrained language models to power our classifiers. \n",
    "Using a pretrained model as input, both means a potentially huge gain in performance, but also a potentially problematic introduction of bias. \n",
    "\n",
    "Since you are not controlling the population / dataset from which your model learns it is hard to guarantee that the models do not come with certain biases builtin. \n",
    "\n",
    "As the pretrained models come \"free\", you should instead spent ressources on investigating and potentially eliminating biases (bias correction). Today you will practice investigating the biases. \n",
    "\n",
    "We will do this using two datasets: \n",
    "1. From the paper:  \"Examining Gender and Race Bias in Two Hundred Sentiment Analysis Systems\" by Kiritchenko & Mohammad 2018:. [data](https://saifmohammad.com/WebDocs/EEC/Equity-Evaluation-Corpus.zip)\n",
    "\n",
    "2. Kaggle Toxicity Classification: https://www.kaggle.com/c/jigsaw-unintended-bias-in-toxicity-classification/data\n",
    "Follow the url. Sign in and download the zip file.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Download the equity evaluation corpus\n",
    "import requests\n",
    "response = requests.get('https://saifmohammad.com/WebDocs/EEC/Equity-Evaluation-Corpus.zip')\n",
    "\n",
    "with open('Equity-Evaluation-Corpus.zip','wb') as f:\n",
    "    f.write(response.content)\n",
    "\n",
    "import zipfile\n",
    "zip_ref = zipfile.ZipFile('Equity-Evaluation-Corpus.zip', 'r')\n",
    "directory_to_extract_to = 'bias_dataset'\n",
    "\n",
    "import os\n",
    "if not os.path.isdir(directory_to_extract_to):\n",
    "    os.mkdir(directory_to_extract_to)\n",
    "zip_ref.extractall(directory_to_extract_to)\n",
    "zip_ref.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "directory_to_extract_to = 'bias_dataset'\n",
    "bias_df = pd.read_csv(directory_to_extract_to+'/Equity-Evaluation-Corpus.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Template</th>\n",
       "      <th>Person</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Race</th>\n",
       "      <th>Emotion</th>\n",
       "      <th>Emotion word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-En-mystery-05498</td>\n",
       "      <td>Alonzo feels angry.</td>\n",
       "      <td>&lt;person subject&gt; feels &lt;emotion word&gt;.</td>\n",
       "      <td>Alonzo</td>\n",
       "      <td>male</td>\n",
       "      <td>African-American</td>\n",
       "      <td>anger</td>\n",
       "      <td>angry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018-En-mystery-11722</td>\n",
       "      <td>Alonzo feels furious.</td>\n",
       "      <td>&lt;person subject&gt; feels &lt;emotion word&gt;.</td>\n",
       "      <td>Alonzo</td>\n",
       "      <td>male</td>\n",
       "      <td>African-American</td>\n",
       "      <td>anger</td>\n",
       "      <td>furious</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018-En-mystery-11364</td>\n",
       "      <td>Alonzo feels irritated.</td>\n",
       "      <td>&lt;person subject&gt; feels &lt;emotion word&gt;.</td>\n",
       "      <td>Alonzo</td>\n",
       "      <td>male</td>\n",
       "      <td>African-American</td>\n",
       "      <td>anger</td>\n",
       "      <td>irritated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2018-En-mystery-14320</td>\n",
       "      <td>Alonzo feels enraged.</td>\n",
       "      <td>&lt;person subject&gt; feels &lt;emotion word&gt;.</td>\n",
       "      <td>Alonzo</td>\n",
       "      <td>male</td>\n",
       "      <td>African-American</td>\n",
       "      <td>anger</td>\n",
       "      <td>enraged</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2018-En-mystery-14114</td>\n",
       "      <td>Alonzo feels annoyed.</td>\n",
       "      <td>&lt;person subject&gt; feels &lt;emotion word&gt;.</td>\n",
       "      <td>Alonzo</td>\n",
       "      <td>male</td>\n",
       "      <td>African-American</td>\n",
       "      <td>anger</td>\n",
       "      <td>annoyed</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      ID                 Sentence  \\\n",
       "0  2018-En-mystery-05498      Alonzo feels angry.   \n",
       "1  2018-En-mystery-11722    Alonzo feels furious.   \n",
       "2  2018-En-mystery-11364  Alonzo feels irritated.   \n",
       "3  2018-En-mystery-14320    Alonzo feels enraged.   \n",
       "4  2018-En-mystery-14114    Alonzo feels annoyed.   \n",
       "\n",
       "                                 Template  Person Gender              Race  \\\n",
       "0  <person subject> feels <emotion word>.  Alonzo   male  African-American   \n",
       "1  <person subject> feels <emotion word>.  Alonzo   male  African-American   \n",
       "2  <person subject> feels <emotion word>.  Alonzo   male  African-American   \n",
       "3  <person subject> feels <emotion word>.  Alonzo   male  African-American   \n",
       "4  <person subject> feels <emotion word>.  Alonzo   male  African-American   \n",
       "\n",
       "  Emotion Emotion word  \n",
       "0   anger        angry  \n",
       "1   anger      furious  \n",
       "2   anger    irritated  \n",
       "3   anger      enraged  \n",
       "4   anger      annoyed  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "bias_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset contains short sentences expressing a simple sentiment, but with changing characters connotating different genders and ethnicities. This allows you to test your classifier in relation to these biases.\n",
    "\n",
    "Today we will test two types of classifiers.\n",
    "\n",
    "- Baseline classifier trained yourself on a given dataset:\n",
    "    - pick either fasttext.\n",
    "    - or the NBLOG (Naive Bayes features feed into a Logistic Regression)\n",
    "    \n",
    "- And the Deepmoji classifier.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the DeepMoji encoder\n",
    "First we shall see what biases the [DeepMoji](https://arxiv.org/pdf/1708.00524.pdf) encoder has out of the box.\n",
    "\n",
    "In this way we get to practice loading and interacting with a pretrained model.\n",
    "\n",
    "DeepMoji was originally conceived using [Keras](https://github.com/bfelbo/DeepMoji), but since you are use to PyTorch we shall use the [TorchMoji](https://github.com/huggingface/torchMoji) implementation.\n",
    "\n",
    "Loading it is straightforward using git."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## clone the repository\n",
    "#! git clone https://github.com/huggingface/torchMoji.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/sh: 1: torchMoji/scripts/download_weights.py: Permission denied\r\n"
     ]
    }
   ],
   "source": [
    "## download the pretrained model's weights using their script\n",
    "! torchMoji/scripts/download_weights.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obtaining file:///home/kristian/Documents/github/tsds/material/13_text3/torchMoji\n",
      "Collecting emoji==0.4.5 (from torchmoji==1.0)\n",
      "  Downloading https://files.pythonhosted.org/packages/7e/0c/c3d24c913986271484fe85446a158ab7b5ff068daa5c2e0ba8793116eed6/emoji-0.4.5.tar.gz\n",
      "Collecting numpy==1.13.1 (from torchmoji==1.0)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/59/e2/57c1a6af4ff0ac095dd68b12bf07771813dbf401faf1b97f5fc0cb963647/numpy-1.13.1-cp36-cp36m-manylinux1_x86_64.whl (17.0MB)\n",
      "\u001b[K     |████████████████████████████████| 17.0MB 6.4MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting scipy==0.19.1 (from torchmoji==1.0)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0e/46/da8d7166102d29695330f7c0b912955498542988542c0d2ae3ea0389c68d/scipy-0.19.1-cp36-cp36m-manylinux1_x86_64.whl (48.2MB)\n",
      "\u001b[K     |████████████████████████████████| 48.2MB 5.4MB/s eta 0:00:01     |██████████████████████          | 33.1MB 5.0MB/s eta 0:00:04\n",
      "\u001b[?25hCollecting scikit-learn==0.19.0 (from torchmoji==1.0)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a4/b3/209652a5d60ce4a2a8a35ad893d7565bbb0f87ce043264ba5c9e7de304cd/scikit_learn-0.19.0-cp36-cp36m-manylinux1_x86_64.whl (12.4MB)\n",
      "\u001b[K     |████████████████████████████████| 12.4MB 5.0MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting text-unidecode==1.0 (from torchmoji==1.0)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/09/c6/c7a477228b2162937f200ece3793bb21c0f21f66b00fc010cdeb93cf465b/text_unidecode-1.0-py2.py3-none-any.whl (75kB)\n",
      "\u001b[K     |████████████████████████████████| 81kB 5.6MB/s eta 0:00:011\n",
      "\u001b[?25hBuilding wheels for collected packages: emoji\n",
      "  Building wheel for emoji (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/kristian/.cache/pip/wheels/82/5f/75/d3b84d3c13409f43533b70af38ca20abb09f7ffb0aaf051e33\n",
      "Successfully built emoji\n",
      "\u001b[31mERROR: tensorflow 1.13.1 has requirement numpy>=1.13.3, but you'll have numpy 1.13.1 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: tensorflow-estimator 1.13.0 has requirement numpy>=1.13.3, but you'll have numpy 1.13.1 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: osmnx 0.9 has requirement numpy>=1.15, but you'll have numpy 1.13.1 which is incompatible.\u001b[0m\n",
      "Installing collected packages: emoji, numpy, scipy, scikit-learn, text-unidecode, torchmoji\n",
      "  Found existing installation: numpy 1.16.2\n",
      "    Uninstalling numpy-1.16.2:\n",
      "      Successfully uninstalled numpy-1.16.2\n",
      "  Found existing installation: scipy 1.1.0\n",
      "    Uninstalling scipy-1.1.0:\n",
      "      Successfully uninstalled scipy-1.1.0\n",
      "  Found existing installation: scikit-learn 0.20.2\n",
      "    Uninstalling scikit-learn-0.20.2:\n",
      "      Successfully uninstalled scikit-learn-0.20.2\n",
      "  Running setup.py develop for torchmoji\n",
      "Successfully installed emoji-0.4.5 numpy-1.13.1 scikit-learn-0.19.0 scipy-1.1.0 text-unidecode-1.0 torchmoji\n",
      "\u001b[33mWARNING: You are using pip version 19.1, however version 19.1.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# navigate to the torchmoji folder\n",
    "import os\n",
    "#os.chdir('torchMoji')\n",
    "## install dependencies\n",
    "! pip install -e .\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If already downloaded elsewhere add the deepmoji directory to the sys.path so python can import it automatically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add to sys.path\n",
    "import sys\n",
    "base_path = '' # change if you have downloaded folder elsewhere.\n",
    "#base_path = '/mnt/b0c8e396-e5ba-4614-be6f-146c4c861ab3/torchMoji/' ## path to the torchmoji directory\n",
    "#sys.path.insert(0, base_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load model and tokenizer\n",
    "from torchmoji.sentence_tokenizer import SentenceTokenizer\n",
    "# load the deepmoji encoder that transforms text to emojies.\n",
    "from torchmoji.model_def import torchmoji_emojis\n",
    "from torchmoji.global_variables import PRETRAINED_PATH, VOCAB_PATH\n",
    "import json,csv, numpy as np\n",
    "import warnings; warnings.simplefilter('ignore')\n",
    "\n",
    "\n",
    "## set the max context length\n",
    "max_token = 30 ## This will not work for longer texts,\n",
    "################# here you should consider splitting each text into smaller segments.\n",
    "\n",
    "# Load vocab (i.e. the index of each word in the vector representation)\n",
    "with open(VOCAB_PATH, 'r') as f:\n",
    "    vocabulary = json.load(f)\n",
    "\n",
    "# initialize tokenizer\n",
    "sentence_tokenizer = SentenceTokenizer(vocabulary, max_token)\n",
    "# load model\n",
    "model = torchmoji_emojis(PRETRAINED_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model outputs a vector of length 64 representing the probability of 64 emojiies.\n",
    "\n",
    "We can find the index of the emojies with descriptions in the data folder.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/emoji_codes.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-cc1de74c11c5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_path\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'data/emoji_codes.json'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0memoji_desc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0memoji_desc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/emoji_codes.json'"
     ]
    }
   ],
   "source": [
    "with open(base_path+'data/emoji_codes.json') as f:\n",
    "    emoji_desc = json.load(f)\n",
    "list(emoji_desc.items())[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We know use this index and the emoji package to translate the index to emojiies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('😂', ':joy:')"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import emoji\n",
    "def translate_emoji(emoji_descr):\n",
    "    if emoji_descr in emoji.unicode_codes.EMOJI_ALIAS_UNICODE:\n",
    "        return emoji.unicode_codes.EMOJI_ALIAS_UNICODE[emoji_descr]\n",
    "    if emoji_descr in emoji.unicode_codes.EMOJI_UNICODE:\n",
    "        return emoji.unicode_codes.EMOJI_UNICODE[emoji_descr]\n",
    "    return emoji_descr\n",
    "to_emoji = [translate_emoji(desc) for i,desc in sorted(emoji_desc.items(),key=lambda x: int(x[0]))]\n",
    "to_emoji_desc = [desc for i,desc in sorted(emoji_desc.items(),key=lambda x: int(x[0]))]\n",
    "\n",
    "## index \n",
    "to_emoji[0],to_emoji_desc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to encode the text as emojis\n",
    "\n",
    "**note we are not using it for transfer learning** but simple as a pretrained classifier.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 13.1.1\n",
    "Use the sentence_tokenizer defined above to tokenize the documents.\n",
    "\n",
    "see example in the torchmoji examples [e.g.](https://github.com/huggingface/torchMoji/blob/master/examples/encode_texts.py) folder for help.\n",
    "\n",
    "Inspect the tokenized documents to see the format. Try to convert them back using <code>vocabulary</code> variable defined earlier.\n",
    "\n",
    "**- Hint this means reversing the vocabulary dictionary.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Answer to ex. 13.1.1. here]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.09 s, sys: 4 ms, total: 1.09 s\n",
      "Wall time: 1.09 s\n"
     ]
    }
   ],
   "source": [
    "docs = bias_df.Sentence.values\n",
    "%time tokenized, _, _ = sentence_tokenizer.tokenize_sentences(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([33306,  1459,  1740,    11,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0], dtype=uint16)"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['alonzo',\n",
       " 'feels',\n",
       " 'angry',\n",
       " '.',\n",
       " 'CUSTOM_MASK',\n",
       " 'CUSTOM_MASK',\n",
       " 'CUSTOM_MASK',\n",
       " 'CUSTOM_MASK',\n",
       " 'CUSTOM_MASK',\n",
       " 'CUSTOM_MASK',\n",
       " 'CUSTOM_MASK',\n",
       " 'CUSTOM_MASK',\n",
       " 'CUSTOM_MASK',\n",
       " 'CUSTOM_MASK',\n",
       " 'CUSTOM_MASK',\n",
       " 'CUSTOM_MASK',\n",
       " 'CUSTOM_MASK',\n",
       " 'CUSTOM_MASK',\n",
       " 'CUSTOM_MASK',\n",
       " 'CUSTOM_MASK',\n",
       " 'CUSTOM_MASK',\n",
       " 'CUSTOM_MASK',\n",
       " 'CUSTOM_MASK',\n",
       " 'CUSTOM_MASK',\n",
       " 'CUSTOM_MASK',\n",
       " 'CUSTOM_MASK',\n",
       " 'CUSTOM_MASK',\n",
       " 'CUSTOM_MASK',\n",
       " 'CUSTOM_MASK',\n",
       " 'CUSTOM_MASK']"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = sorted(vocabulary,key=lambda x: vocabulary[x])\n",
    "[vocab[i] for i in tokenized[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 13.1.2\n",
    "Encode the tokenized sentences and wrap it in a function.\n",
    "- Hint: Do a forward pass of the model on the tokenized data.\n",
    "    - check [here](https://github.com/huggingface/torchMoji/blob/master/examples/encode_texts.py) for help \n",
    "\n",
    "For larger datasets and with longer sentences encoding is problematic if not done in batches. \n",
    "\n",
    "Write a for loop that takes only 256 tokenized documents at a time and concatenate them to a dataframe in the end.\n",
    "\n",
    "Use the <code>to_emoji</code> list as columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Answer to ex 13.1.2 here]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 256/256 [00:12<00:00, 22.83it/s]\n"
     ]
    }
   ],
   "source": [
    "n_batch = 256\n",
    "bs = len(tokenized)//n_batch\n",
    "enc = []\n",
    "import tqdm\n",
    "import pandas as pd\n",
    "def emoji_encode(tokenized,to_df=False):\n",
    "    #tokenized, _, _ = sentence_tokenizer.tokenize_sentences(texts)\n",
    "    probs = model(tokenized)\n",
    "    if to_df:\n",
    "        return pd.DataFrame(probs,columns=to_emoji_desc)\n",
    "    return probs\n",
    "\n",
    "for i in tqdm.tqdm(list(range(n_batch))):\n",
    "    batch = tokenized[i*bs:(i+1)*bs]\n",
    "    emoji_encoded = emoji_encode(batch,to_df=True)\n",
    "    enc.append(emoji_encoded)\n",
    "batch = tokenized[(i+1)*bs:]\n",
    "emoji_encoded = emoji_encode(batch,to_df=True)\n",
    "enc.append(emoji_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "emoji_df = pd.concat(enc)\n",
    "emoji_df.columns = to_emoji"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ex. 13.1.3\n",
    "- Join the output of Deepmoji with the bias dataframe columns (Race, Gender and Emotion)\n",
    "    - Make sure Race count and Gender counts are equal after join.\n",
    "\n",
    "Investigate if there are significant differences in relation to **Race** (Race column).\n",
    "\n",
    "- See which types of emojies are most different.\n",
    "\n",
    "\n",
    "- Make a dictionary mapping emojiis to different classes, either happy, sad, angry etc.\n",
    "- HINT: Fastests way is to loop through the list of emojiies <code>to_emoji</code> and use the <code>input()</code> to input the class.\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Answer to ex. 13.1.3 here]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [This question is in assignment 4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ex.13.1.4 - See which emotions are most biased\n",
    "\n",
    "- Groupby Emotion and Race and calculate absolute difference in emoji encoding. \n",
    "    - hint: first groupby emotion and race, calculate mean, then diff, then abs and then sum.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Answer to ex. 13.1.4 here]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Emotion  Race            \n",
       "anger    African-American    0.000000\n",
       "         European            0.096118\n",
       "fear     African-American    0.000000\n",
       "         European            0.105285\n",
       "joy      African-American    0.000000\n",
       "         European            0.092333\n",
       "sadness  African-American    0.000000\n",
       "         European            0.086262\n",
       "dtype: float32"
      ]
     },
     "execution_count": 299,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emoji_df.groupby(['Emotion','Race']).mean().groupby('Emotion').diff().apply(abs).sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Encode \n",
    "def deepmoji_encode(tokenized):\n",
    "    ## let the forward pass end before the softmax layer.\n",
    "    model.feature_output = True\n",
    "    ### Do forward pass.\n",
    "    probs = model(tokenized)\n",
    "    ## set the model back to default emoji output.\n",
    "    model.feature_output = False\n",
    "    return probs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hostility and Minority Dataset (Kaggle Toxicity Classification)\n",
    "**Context**\n",
    "All outcome and minority variables are crowdsourced annotations and variables are expressed and percentage of annotators marking the category. This means that to create categorical outcomes we should apply a cutoff. The dataset provider suggests 0.5. \n",
    "\n",
    "**Ex.13.2.1:** \n",
    "- Define a variable <code>minority_cols</code> as a list of column names of the minorities.\n",
    "- Define a variable <code>outcome_cols</code> as a list of column names of the minorities.\n",
    "- Create a categorical version of each variable in the outcome cols and minority cols.\n",
    "\n",
    "\n",
    "**Ex. 13.2.2:** The dataset is fairly large so subsampling will be a good idea (e.g. 25000 samples) where minorities are upsampled. Do a subsample of 1000 for each minority including a none category. \n",
    "\n",
    "**Ex 13.2.3:** Train one of the baseline classifiers (see lecture 13 for bow based and fasttext) on the hostility and minority dataset.\n",
    "\n",
    "**Ex.13.2.4:** Investigate biases in relation to minority groups.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Answers to ex 13.2.x here]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bow baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "with open('get_bow_baseline.py','w') as f:\n",
    "    f.write(requests.get('https://raw.githubusercontent.com/snorreralund/test_tokenization/master/get_bow_baseline.py').text)\n",
    "f.close()\n",
    "import get_bow_baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "toxicity_df = pd.read_csv('/home/snorre/Dropbox/Forskning/PhD/undervisning/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>target</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>severe_toxicity</th>\n",
       "      <th>obscene</th>\n",
       "      <th>identity_attack</th>\n",
       "      <th>insult</th>\n",
       "      <th>threat</th>\n",
       "      <th>asian</th>\n",
       "      <th>atheist</th>\n",
       "      <th>...</th>\n",
       "      <th>article_id</th>\n",
       "      <th>rating</th>\n",
       "      <th>funny</th>\n",
       "      <th>wow</th>\n",
       "      <th>sad</th>\n",
       "      <th>likes</th>\n",
       "      <th>disagree</th>\n",
       "      <th>sexual_explicit</th>\n",
       "      <th>identity_annotator_count</th>\n",
       "      <th>toxicity_annotator_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>59848</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>This is so cool. It's like, 'would you want yo...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>2006</td>\n",
       "      <td>rejected</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>59849</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Thank you!! This would make my life a lot less...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>2006</td>\n",
       "      <td>rejected</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>59852</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>This is such an urgent design problem; kudos t...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>2006</td>\n",
       "      <td>rejected</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>59855</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Is this something I'll be able to install on m...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>2006</td>\n",
       "      <td>rejected</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>59856</td>\n",
       "      <td>0.893617</td>\n",
       "      <td>haha you guys are a bunch of losers.</td>\n",
       "      <td>0.021277</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.021277</td>\n",
       "      <td>0.87234</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2006</td>\n",
       "      <td>rejected</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 45 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      id    target                                       comment_text  \\\n",
       "0  59848  0.000000  This is so cool. It's like, 'would you want yo...   \n",
       "1  59849  0.000000  Thank you!! This would make my life a lot less...   \n",
       "2  59852  0.000000  This is such an urgent design problem; kudos t...   \n",
       "3  59855  0.000000  Is this something I'll be able to install on m...   \n",
       "4  59856  0.893617               haha you guys are a bunch of losers.   \n",
       "\n",
       "   severe_toxicity  obscene  identity_attack   insult  threat  asian  atheist  \\\n",
       "0         0.000000      0.0         0.000000  0.00000     0.0    NaN      NaN   \n",
       "1         0.000000      0.0         0.000000  0.00000     0.0    NaN      NaN   \n",
       "2         0.000000      0.0         0.000000  0.00000     0.0    NaN      NaN   \n",
       "3         0.000000      0.0         0.000000  0.00000     0.0    NaN      NaN   \n",
       "4         0.021277      0.0         0.021277  0.87234     0.0    0.0      0.0   \n",
       "\n",
       "   ...  article_id    rating  funny  wow  sad  likes  disagree  \\\n",
       "0  ...        2006  rejected      0    0    0      0         0   \n",
       "1  ...        2006  rejected      0    0    0      0         0   \n",
       "2  ...        2006  rejected      0    0    0      0         0   \n",
       "3  ...        2006  rejected      0    0    0      0         0   \n",
       "4  ...        2006  rejected      0    0    0      1         0   \n",
       "\n",
       "   sexual_explicit  identity_annotator_count  toxicity_annotator_count  \n",
       "0              0.0                         0                         4  \n",
       "1              0.0                         0                         4  \n",
       "2              0.0                         0                         4  \n",
       "3              0.0                         0                         4  \n",
       "4              0.0                         4                        47  \n",
       "\n",
       "[5 rows x 45 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toxicity_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'target', 'comment_text', 'severe_toxicity', 'obscene',\n",
       "       'identity_attack', 'insult', 'threat', 'asian', 'atheist', 'bisexual',\n",
       "       'black', 'buddhist', 'christian', 'female', 'heterosexual', 'hindu',\n",
       "       'homosexual_gay_or_lesbian', 'intellectual_or_learning_disability',\n",
       "       'jewish', 'latino', 'male', 'muslim', 'other_disability',\n",
       "       'other_gender', 'other_race_or_ethnicity', 'other_religion',\n",
       "       'other_sexual_orientation', 'physical_disability',\n",
       "       'psychiatric_or_mental_illness', 'transgender', 'white', 'created_date',\n",
       "       'publication_id', 'parent_id', 'article_id', 'rating', 'funny', 'wow',\n",
       "       'sad', 'likes', 'disagree', 'sexual_explicit',\n",
       "       'identity_annotator_count', 'toxicity_annotator_count'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toxicity_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "asian                                   4846.792005\n",
       "atheist                                 1298.449450\n",
       "bisexual                                 763.380519\n",
       "black                                  13933.484260\n",
       "buddhist                                 571.434540\n",
       "christian                              38595.950842\n",
       "female                                 51723.057378\n",
       "heterosexual                            1311.421852\n",
       "hindu                                    590.427372\n",
       "homosexual_gay_or_lesbian              10375.613491\n",
       "intellectual_or_learning_disability      440.822098\n",
       "jewish                                  7236.672289\n",
       "latino                                  2482.062856\n",
       "male                                   44032.257970\n",
       "muslim                                 20037.556177\n",
       "physical_disability                      549.380996\n",
       "psychiatric_or_mental_illness           4895.233197\n",
       "transgender                             2723.960538\n",
       "white                                  23072.311802\n",
       "dtype: float64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "minority_columns = ['asian', 'atheist', 'bisexual',\n",
    "       'black', 'buddhist', 'christian', 'female', 'heterosexual', 'hindu',\n",
    "       'homosexual_gay_or_lesbian', 'intellectual_or_learning_disability',\n",
    "       'jewish', 'latino', 'male', 'muslim', 'physical_disability',\n",
    "       'psychiatric_or_mental_illness', 'transgender', 'white'\n",
    "                    #, 'other_disability',\n",
    "       #'other_gender', 'other_race_or_ethnicity', 'other_religion',\n",
    "       #'other_sexual_orientation',\n",
    "                   ]\n",
    "toxicity_df[minority_columns].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>asian</th>\n",
       "      <th>atheist</th>\n",
       "      <th>bisexual</th>\n",
       "      <th>black</th>\n",
       "      <th>buddhist</th>\n",
       "      <th>christian</th>\n",
       "      <th>female</th>\n",
       "      <th>heterosexual</th>\n",
       "      <th>hindu</th>\n",
       "      <th>homosexual_gay_or_lesbian</th>\n",
       "      <th>intellectual_or_learning_disability</th>\n",
       "      <th>jewish</th>\n",
       "      <th>latino</th>\n",
       "      <th>male</th>\n",
       "      <th>muslim</th>\n",
       "      <th>physical_disability</th>\n",
       "      <th>psychiatric_or_mental_illness</th>\n",
       "      <th>transgender</th>\n",
       "      <th>white</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   asian  atheist  bisexual  black  buddhist  christian  female  heterosexual  \\\n",
       "0    NaN      NaN       NaN    NaN       NaN        NaN     NaN           NaN   \n",
       "1    NaN      NaN       NaN    NaN       NaN        NaN     NaN           NaN   \n",
       "2    NaN      NaN       NaN    NaN       NaN        NaN     NaN           NaN   \n",
       "3    NaN      NaN       NaN    NaN       NaN        NaN     NaN           NaN   \n",
       "4    0.0      0.0       0.0    0.0       0.0        0.0     0.0           0.0   \n",
       "\n",
       "   hindu  homosexual_gay_or_lesbian  intellectual_or_learning_disability  \\\n",
       "0    NaN                        NaN                                  NaN   \n",
       "1    NaN                        NaN                                  NaN   \n",
       "2    NaN                        NaN                                  NaN   \n",
       "3    NaN                        NaN                                  NaN   \n",
       "4    0.0                        0.0                                 0.25   \n",
       "\n",
       "   jewish  latino  male  muslim  physical_disability  \\\n",
       "0     NaN     NaN   NaN     NaN                  NaN   \n",
       "1     NaN     NaN   NaN     NaN                  NaN   \n",
       "2     NaN     NaN   NaN     NaN                  NaN   \n",
       "3     NaN     NaN   NaN     NaN                  NaN   \n",
       "4     0.0     0.0   0.0     0.0                  0.0   \n",
       "\n",
       "   psychiatric_or_mental_illness  transgender  white  \n",
       "0                            NaN          NaN    NaN  \n",
       "1                            NaN          NaN    NaN  \n",
       "2                            NaN          NaN    NaN  \n",
       "3                            NaN          NaN    NaN  \n",
       "4                            0.0          0.0    0.0  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toxicity_df[minority_columns].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "## downsample the data\n",
    "samples = []\n",
    "for i in minority_columns:\n",
    "    sub = toxicity_df[toxicity_df[i]>0.5]\n",
    "    samples.append(sub.sample(min(len(sub),2500)))\n",
    "samples.append(toxicity_df[toxicity_df[minority_columns].apply(np.isfinite,axis=1).sum(axis=1)==0].sample(10000))\n",
    "subsample = pd.concat(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "asian                                  2592.598986\n",
       "atheist                                1276.641091\n",
       "bisexual                                365.102540\n",
       "black                                  4353.551327\n",
       "buddhist                                665.379257\n",
       "christian                              5829.283233\n",
       "female                                 6547.743259\n",
       "heterosexual                           1199.917006\n",
       "hindu                                   657.067270\n",
       "homosexual_gay_or_lesbian              4178.218788\n",
       "intellectual_or_learning_disability      77.428542\n",
       "jewish                                 3346.776099\n",
       "latino                                 1634.154292\n",
       "male                                   6490.994750\n",
       "muslim                                 4595.055830\n",
       "physical_disability                     113.655244\n",
       "psychiatric_or_mental_illness          2284.221359\n",
       "transgender                            2393.904706\n",
       "white                                  5083.138263\n",
       "dtype: float64"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subsample[minority_columns].sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [],
   "source": [
    "#(toxicity_df[outcome_columns]>0).sum(axis=1).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['0.0', '0.0', '0.3', ..., '0.2', '0.732394366197183', '0.0'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 312,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#toxicity_df[outcome_columns].astype(str)\n",
    "#(toxicity_df[outcome_columns]>0.5).sum(axis=1).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "outcome_columns = np.array(outcome_columns)\n",
    "cutoff = 0.5\n",
    "subsample['y'] = ((subsample['target']>=0.5)*1).astype(str)\n",
    "subsample['y2'] = ((subsample['target']>=0.5)*1)\n",
    "#subsample['y'] = subsample[outcome_columns].apply(lambda x: list(outcome_columns[x>=cutoff]) if sum(x>=cutoff)>0 else ['none'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "subsample = subsample.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make train test split\n",
    "\n",
    "p = 0.5 # split 50-50 because we are equally interested in the test\n",
    "n = int(len(subsample)*p)\n",
    "idx = np.random.permutation(np.arange(len(subsample)))\n",
    "train,test = idx[0:n],idx[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = subsample.iloc[train]\n",
    "test_df = subsample.iloc[test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>target</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>severe_toxicity</th>\n",
       "      <th>obscene</th>\n",
       "      <th>identity_attack</th>\n",
       "      <th>insult</th>\n",
       "      <th>threat</th>\n",
       "      <th>asian</th>\n",
       "      <th>atheist</th>\n",
       "      <th>...</th>\n",
       "      <th>rating</th>\n",
       "      <th>funny</th>\n",
       "      <th>wow</th>\n",
       "      <th>sad</th>\n",
       "      <th>likes</th>\n",
       "      <th>disagree</th>\n",
       "      <th>sexual_explicit</th>\n",
       "      <th>identity_annotator_count</th>\n",
       "      <th>toxicity_annotator_count</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>22241</th>\n",
       "      <td>5366022</td>\n",
       "      <td>0.0</td>\n",
       "      <td>This man who is the supposed Governor can not ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>approved</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10163</th>\n",
       "      <td>6312161</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Easy enough to google. \\n\\n\\nOnce all the wome...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>approved</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1206</th>\n",
       "      <td>5188598</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Two pictures seen in G&amp;M articles about campus...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>approved</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27386</th>\n",
       "      <td>356884</td>\n",
       "      <td>0.0</td>\n",
       "      <td>And there is recourse to those harmed by the h...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>approved</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27863</th>\n",
       "      <td>5690588</td>\n",
       "      <td>0.0</td>\n",
       "      <td>A guy from my hometown served in the military ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>approved</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 46 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            id  target                                       comment_text  \\\n",
       "22241  5366022     0.0  This man who is the supposed Governor can not ...   \n",
       "10163  6312161     0.0  Easy enough to google. \\n\\n\\nOnce all the wome...   \n",
       "1206   5188598     0.0  Two pictures seen in G&M articles about campus...   \n",
       "27386   356884     0.0  And there is recourse to those harmed by the h...   \n",
       "27863  5690588     0.0  A guy from my hometown served in the military ...   \n",
       "\n",
       "       severe_toxicity  obscene  identity_attack  insult  threat  asian  \\\n",
       "22241              0.0      0.0              0.0     0.0     0.0    0.0   \n",
       "10163              0.0      0.0              0.0     0.0     0.0    0.0   \n",
       "1206               0.0      0.0              0.0     0.0     0.0    1.0   \n",
       "27386              0.0      0.0              0.0     0.0     0.0    0.0   \n",
       "27863              0.0      0.0              0.0     0.0     0.0    0.0   \n",
       "\n",
       "       atheist  ...    rating  funny  wow  sad  likes  disagree  \\\n",
       "22241      0.0  ...  approved      0    0    0      6         1   \n",
       "10163      0.0  ...  approved      0    0    0      2         0   \n",
       "1206       0.0  ...  approved      0    0    0      5         1   \n",
       "27386      0.0  ...  approved      0    0    0      3         0   \n",
       "27863      0.0  ...  approved      0    0    0      0         0   \n",
       "\n",
       "       sexual_explicit  identity_annotator_count  toxicity_annotator_count  y  \n",
       "22241              0.0                        10                         4  0  \n",
       "10163              0.0                         4                         4  0  \n",
       "1206               0.0                         4                         4  0  \n",
       "27386              0.0                         4                         4  0  \n",
       "27863              0.0                         4                         4  0  \n",
       "\n",
       "[5 rows x 46 columns]"
      ]
     },
     "execution_count": 335,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! cp /home/snorre/Dropbox/Forskning/PhD/logbog/get_bow_baseline.py ./\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [],
   "source": [
    "%aimport get_bow_baseline\n",
    "import get_bow_baseline\n",
    "import nltk\n",
    "tokenizer = nltk.tokenize.TweetTokenizer().tokenize\n",
    "baseline = get_bow_baseline.TokenizationTest(train_df,test_df,text_col='comment_text',y_col='y2',MAX_EVALS=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 2/50 [00:05<02:22,  2.98s/it, best loss: 0.3039770523080001]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/snorre/anaconda3/envs/env_full/lib/python3.6/site-packages/sklearn/svm/base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 10%|█         | 5/50 [00:18<02:41,  3.59s/it, best loss: 0.2835911275162999]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/snorre/anaconda3/envs/env_full/lib/python3.6/site-packages/sklearn/svm/base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "\n",
      "/home/snorre/anaconda3/envs/env_full/lib/python3.6/site-packages/sklearn/svm/base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 11/50 [00:38<02:07,  3.28s/it, best loss: 0.2835911275162999]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/snorre/anaconda3/envs/env_full/lib/python3.6/site-packages/sklearn/svm/base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 30%|███       | 15/50 [00:54<02:05,  3.58s/it, best loss: 0.2835911275162999]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/snorre/anaconda3/envs/env_full/lib/python3.6/site-packages/sklearn/svm/base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "\n",
      "/home/snorre/anaconda3/envs/env_full/lib/python3.6/site-packages/sklearn/svm/base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "\n",
      "/home/snorre/anaconda3/envs/env_full/lib/python3.6/site-packages/sklearn/svm/base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "\n",
      "/home/snorre/anaconda3/envs/env_full/lib/python3.6/site-packages/sklearn/svm/base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      " 32%|███▏      | 16/50 [01:01<02:33,  4.52s/it, best loss: 0.2835911275162999]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/snorre/anaconda3/envs/env_full/lib/python3.6/site-packages/sklearn/svm/base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "\n",
      "/home/snorre/anaconda3/envs/env_full/lib/python3.6/site-packages/sklearn/svm/base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "\n",
      "/home/snorre/anaconda3/envs/env_full/lib/python3.6/site-packages/sklearn/svm/base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "\n",
      "/home/snorre/anaconda3/envs/env_full/lib/python3.6/site-packages/sklearn/svm/base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      " 34%|███▍      | 17/50 [01:07<02:46,  5.04s/it, best loss: 0.2835911275162999]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/snorre/anaconda3/envs/env_full/lib/python3.6/site-packages/sklearn/svm/base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "\n",
      "/home/snorre/anaconda3/envs/env_full/lib/python3.6/site-packages/sklearn/svm/base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      " 36%|███▌      | 18/50 [01:11<02:36,  4.88s/it, best loss: 0.2835911275162999]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/snorre/anaconda3/envs/env_full/lib/python3.6/site-packages/sklearn/svm/base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 30/50 [01:58<01:12,  3.65s/it, best loss: 0.27989279730057015]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/snorre/anaconda3/envs/env_full/lib/python3.6/site-packages/sklearn/svm/base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "\n",
      "/home/snorre/anaconda3/envs/env_full/lib/python3.6/site-packages/sklearn/svm/base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 68%|██████▊   | 34/50 [02:13<00:57,  3.56s/it, best loss: 0.27989279730057015]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/snorre/anaconda3/envs/env_full/lib/python3.6/site-packages/sklearn/svm/base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "\n",
      "/home/snorre/anaconda3/envs/env_full/lib/python3.6/site-packages/sklearn/svm/base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 84%|████████▍ | 42/50 [02:43<00:28,  3.57s/it, best loss: 0.27989279730057015]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/snorre/anaconda3/envs/env_full/lib/python3.6/site-packages/sklearn/svm/base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 45/50 [02:56<00:20,  4.07s/it, best loss: 0.27868571792003527]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/snorre/anaconda3/envs/env_full/lib/python3.6/site-packages/sklearn/svm/base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "\n",
      "/home/snorre/anaconda3/envs/env_full/lib/python3.6/site-packages/sklearn/svm/base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "\n",
      "/home/snorre/anaconda3/envs/env_full/lib/python3.6/site-packages/sklearn/svm/base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "\n",
      "/home/snorre/anaconda3/envs/env_full/lib/python3.6/site-packages/sklearn/svm/base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 98%|█████████▊| 49/50 [03:12<00:03,  3.98s/it, best loss: 0.27868571792003527]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/snorre/anaconda3/envs/env_full/lib/python3.6/site-packages/sklearn/svm/base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "\n",
      "/home/snorre/anaconda3/envs/env_full/lib/python3.6/site-packages/sklearn/svm/base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "\n",
      "/home/snorre/anaconda3/envs/env_full/lib/python3.6/site-packages/sklearn/svm/base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "\n",
      "/home/snorre/anaconda3/envs/env_full/lib/python3.6/site-packages/sklearn/svm/base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [03:18<00:00,  4.59s/it, best loss: 0.27868571792003527]\n",
      "Final accuracy and roc_auc score of tokenizer (nltk_tweet) + nb_log: 0.862 and 0.811\n"
     ]
    }
   ],
   "source": [
    "baseline.evaluate('nltk_tweet',tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['true_pred'] = baseline.clf.predict(baseline.x_test)==baseline.y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "asian 1504 0.18617021276595744\n",
      "atheist 765 0.1738562091503268\n",
      "bisexual 152 0.21052631578947367\n",
      "black 2363 0.32035548032162503\n",
      "buddhist 401 0.09725685785536159\n",
      "christian 3236 0.1572929542645241\n",
      "female 3447 0.20336524514070206\n",
      "heterosexual 729 0.24828532235939643\n",
      "hindu 370 0.10810810810810811\n",
      "homosexual_gay_or_lesbian 2338 0.3109495295124038\n",
      "intellectual_or_learning_disability 30 0.13333333333333333\n",
      "jewish 1890 0.20317460317460317\n",
      "latino 925 0.2064864864864865\n",
      "male 3490 0.22578796561604583\n",
      "muslim 2516 0.2484101748807631\n",
      "physical_disability 43 0.09302325581395349\n",
      "psychiatric_or_mental_illness 1354 0.2651403249630724\n",
      "transgender 1284 0.24065420560747663\n",
      "white 2840 0.2992957746478873\n"
     ]
    }
   ],
   "source": [
    "for minority in minority_columns:\n",
    "    idx = test_df[minority]>=0.5\n",
    "    \n",
    "    print(minority,sum(idx),sum(test_df[idx]['true_pred'])/sum(idx))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fasttext baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "# Fasttext needs a format where each label is in the beginning of the row and marked with :\n",
    "## __label__{name}\n",
    "def get_labels(val):\n",
    "    if type(val)==str:\n",
    "        return '__label__%s'%val\n",
    "    else:\n",
    "        labels = []\n",
    "        for i in val:\n",
    "            labels.append('__label__%s'%i)\n",
    "        return ' '.join(labels)\n",
    "def make_fasttext_format(df,y_col,text_col,outfile,tokenizer=nltk.tokenize.TweetTokenizer().tokenize):\n",
    "    docs = df[text_col].values\n",
    "    # tokenize\n",
    "    tokenized = [' '.join(tokenizer(doc)) for doc in docs]\n",
    "    # lower\n",
    "    tokenized = [doc.lower().replace('\\n',' __newline__ ') for doc in tokenized]\n",
    "    if type(df[y_col].values[0])==str:\n",
    "        fasttext_labels = ['__label__%s'%val for val in df[y_col]]\n",
    "    else:\n",
    "        fasttext_labels = [get_labels(vals) for vals in df[y_col]]\n",
    "    fast_docs = [' '.join([fasttext_labels[i],tokenized[i]]) for i in range(len(df))]\n",
    "    with open(outfile,'w') as f:\n",
    "        f.write('\\n'.join(fast_docs))\n",
    "    f.close()\n",
    "\n",
    "y_col =  'y' \n",
    "text_col = 'comment_text'\n",
    "trainfile = 'hostility.train'\n",
    "testfile = 'hostility.test'\n",
    "make_fasttext_format(train_df,\n",
    "                     y_col=y_col,\n",
    "                     text_col=text_col\n",
    "                     ,outfile=trainfile)\n",
    "\n",
    "make_fasttext_format(test_df,\n",
    "                     y_col=y_col,\n",
    "                     text_col=text_col\n",
    "                     ,outfile=testfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit the model and test it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read 1M words\n",
      "Number of words:  48311\n",
      "Number of labels: 2\n",
      "Progress: 100.0% words/sec/thread: 1935159 lr:  0.000000 loss:  0.024559 ETA:   0h 0m\n",
      "N\t21058\n",
      "P@1\t0.856\n",
      "R@1\t0.856\n"
     ]
    }
   ],
   "source": [
    "fast_path = '/mnt/b0c8e396-e5ba-4614-be6f-146c4c861ab3/fastText-0.2.0'\n",
    "! {fast_path}/./fasttext supervised -input hostility.train -output model_fast -lr 0.5 -epoch 100 -wordNgrams 2 -dim 10 -ws 5\n",
    "#! {fast_path}/./fasttext supervised -input fasttext.train -output model_fast -lr 0.5 -epoch 200 -wordNgrams 3 -dim 100 -ws 5\n",
    "! {fast_path}/./fasttext test model_fast.bin hostility.test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_classes = len(outcome_columns)+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "! {fast_path}/./fasttext predict-prob model_fast.bin hostility.test {nb_classes}  > predict_proba.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "! {fast_path}/./fasttext predict model_fast.bin hostility.test 1  > predict.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__label__none\r\n",
      "__label__none\r\n",
      "__label__none\r\n",
      "__label__none\r\n",
      "__label__insult\r\n",
      "__label__identity_attack\r\n",
      "__label__none\r\n",
      "__label__none\r\n",
      "__label__insult\r\n",
      "__label__none\r\n"
     ]
    }
   ],
   "source": [
    "! tail predict.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_fasttext_row(row):\n",
    "    labels = row.split('__label__')[1:]\n",
    "    return dict(zip(labels,[1 for i in range(len(labels))]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_fasttext_row_prob(row):\n",
    "    preds = {}\n",
    "    for pair in row.split('__label__')[1:]:\n",
    "        i,j = pair.split()\n",
    "        preds[i.strip()] = float(j.strip())\n",
    "        \n",
    "    return preds\n",
    "    \n",
    "def load_fasttext_proba(predict_file):\n",
    "    \n",
    "    with open(predict_file,'r') as f:\n",
    "        l = f.read().split('\\n')\n",
    "    return pd.DataFrame([read_fasttext_row_prob(i) for i in l])\n",
    "#! tail predict_proba.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_fasttext_predict(predict_file):\n",
    "    \n",
    "    with open(predict_file,'r') as f:\n",
    "        l = f.read().split('\\n')\n",
    "    return pd.DataFrame([read_fasttext_row(i) for i in l])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict = load_fasttext_predict('predict.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus \n",
    "So far we have not done any real transfer learning. For this exercise you should visit some of the major models and investigate how to adopt the model to your own dataset.\n",
    "\n",
    "BERT - [COLAB Example](https://colab.research.google.com/github/tensorflow/tpu/blob/master/tools/colab/bert_finetuning_with_cloud_tpus.ipynb)\n",
    "\n",
    "DeepMoji:\n",
    "Try the deepmoji finetuning example [here](https://colab.research.google.com/drive/1IsV5a_tr2c5OVdKnX_PyGjoRWW8DUG0S)\n",
    "    - HINT: Inspect the load_benchmark data to see how to make your own dataset conform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Answer to bonus question here]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
