{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "! cp lexi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook for collecting and compilling lexicons for text mining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Argument dictionary\n",
    "*Swapna Somasundaran, Josef Ruppenhofer and Janyce Wiebe (2007) Detecting Arguing and Sentiment in Meetings, SIGdial Workshop on Discourse and Dialogue, Antwerp, Belgium, September 2007 (SIGdial Workshop 2007).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 636,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'inconsistency': 18,\n",
       " 'conditionals': 8,\n",
       " 'contrast': 12,\n",
       " 'emphasis': 30,\n",
       " 'causation': 38,\n",
       " 'wants': 6,\n",
       " 'difficulty': 11,\n",
       " 'inyourshoes': 4,\n",
       " 'rhetoricalquestion': 5,\n",
       " 'assessments': 24,\n",
       " 'generalization': 5,\n",
       " 'structure': 3,\n",
       " 'necessity': 25,\n",
       " 'doubt': 4,\n",
       " 'priority': 8,\n",
       " 'possibility': 21,\n",
       " 'authority': 1}"
      ]
     },
     "execution_count": 636,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import os,re\n",
    "path = '/home/snorre/Dropbox/Forskning/PhD/undervisning/arglex_Somasundaran07/arglex_Somasundaran07/'\n",
    "files = [path+i for i in os.listdir(path) if 'tff' in i]\n",
    "macros = ['modals.tff','spoken.tff',\n",
    "'wordclasses.tff',\n",
    "'pronoun.tff','intensifiers.tff']\n",
    "macro2replace = {}\n",
    "for macro in macros:\n",
    "    filename = path+macro\n",
    "    l = open(filename,'r').read().split('\\n')[0:-1]\n",
    "    for i in l[1:]:\n",
    "        name = i.split('={')[0]\n",
    "        if not '@' in name:\n",
    "            continue\n",
    "        words = '|'.join(i.split('={')[1].strip('}').split(','))\n",
    "        macro2replace[name] = words\n",
    "        \n",
    "class2re = {}\n",
    "for filename in files:\n",
    "    if filename.split('/')[-1] in macros:\n",
    "        continue\n",
    "    l = open(filename,'r').read().split('\\n')[0:-1]\n",
    "    name = l[0].split('\"')[1]\n",
    "    \n",
    "    #print(name,len(class2re),end=' ')\n",
    "    expressions = l[1:]\n",
    "    expand_exp = []\n",
    "    for exp in expressions:\n",
    "        for macro,rep in sorted(macro2replace.items(),key=lambda x: len(x[0]),reverse=True):\n",
    "            if macro in exp:\n",
    "                exp = exp.replace(macro,rep)\n",
    "        if exp=='':\n",
    "            continue\n",
    "        expand_exp.append(exp)\n",
    "    re_exp = re.compile('|'.join(expand_exp),flags=re.IGNORECASE)\n",
    "    #print(class2re)\n",
    "    class2re[name] = re_exp\n",
    "    \n",
    "def text2argfeatures(text):\n",
    "    d = {}\n",
    "    for name,regex in class2re.items():\n",
    "        d[name] = len(regex.findall(text))\n",
    "    return d\n",
    "#import codecs\n",
    "string_test = codecs.open(path+'patterntest','r','utf-8').read()\n",
    "\n",
    "import pickle\n",
    "pickle.dump([class2re,string_test],open('lexicon_functions/text2arg.pkl','wb'))\n",
    "class2re,string_test = pickle.load(open('lexicon_functions/text2arg.pkl','rb'))\n",
    "text2argfeatures(string_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subjectivity\n",
    "http://mpqa.cs.pitt.edu/lexicons/subj_lexicon/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/home/snorre/Dropbox/Forskning/PhD/undervisning/subjectivity_clues_hltemnlp05/subjclueslen1-HLTEMNLP05.tff'\n",
    "\n",
    "l = open(path,'r').read().split('\\n')\n",
    "\n",
    "data = []\n",
    "for val in l[0:-1]:\n",
    "    typ = val.split()[0].split('=')[1]\n",
    "    length = int(val.split()[1].split('=')[1])\n",
    "    word = ' '.join(val.split()[2:2+length]).split('=')[1]\n",
    "    vals = val.split()[2+length:]\n",
    "    d = dict([i.split('=') for i in vals if len(i.split('='))==2])\n",
    "    d.update({'w':word,'length':length,'type':typ})\n",
    "    data.append(d)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 635,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'weaksubj': 0.0, 'strongsubj': 0.09090909090909091}"
      ]
     },
     "execution_count": 635,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "df = pd.DataFrame(data)\n",
    "df.sample(5)\n",
    "subjectivity_types = list(df.type.unique())\n",
    "w2subj = dict(df[['w','type']].values)\n",
    "def get_subjectivity(doc, tokenizer=nltk.word_tokenize,agg='mean'):\n",
    "    if type(doc)==str:\n",
    "        doc = tokenizer(doc)\n",
    "    assert type(doc)==list,\"please input either a list or a string\"\n",
    "    if len(doc)==0:\n",
    "        return np.nan\n",
    "    matches = Counter()\n",
    "    for w in doc:\n",
    "        w = w.lower()\n",
    "        if w in w2subj:\n",
    "            matches[w2subj[w]]+=1\n",
    "    \n",
    "    if len(matches)==0:\n",
    "        return {typ:0 for typ in subjectivity_types}\n",
    "    scores = pd.Series(np.array([matches[typ] for typ in subjectivity_types]),index=subjectivity_types)\n",
    "    if agg=='mean':\n",
    "        scores =  scores/len(doc)\n",
    "    elif agg =='abs': \n",
    "        scores = scores\n",
    "    else:\n",
    "        scores =  agg(scores)\n",
    "    return dict(scores)\n",
    "\n",
    "pickle.dump([w2subj,subjectivity_types],open('lexicon_functions/subjectivity_score.pkl','wb'))\n",
    "w2subj,subjectivity_types = pickle.load(open('lexicon_functions/subjectivity_score.pkl','rb'))\n",
    "get_subjectivity('absolutely, i once kissed a girl and i liked it')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vader Sentiment\n",
    "\n",
    "*Hutto, C.J. & Gilbert, E.E. (2014). VADER: A Parsimonious Rule-based Model for Sentiment Analysis of Social Media Text. Eighth International Conference on Weblogs and Social Media (ICWSM-14). Ann Arbor, MI, June 2014.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk.sentiment\n",
    "vader = nltk.sentiment.vader.SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vader.polarity_scores('Hello everybody. Nothing to see here.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NRC\n",
    "\n",
    "** Not for commercial use ** \n",
    "http://sentiment.nrc.ca/lexicons-for-research/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! wget http://sentiment.nrc.ca/lexicons-for-research/NRC-Sentiment-Emotion-Lexicons.zip\n",
    "#! unzip NRC-Sentiment-Emotion-Lexicons.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! ls NRC-Sentiment-Emotion-Lexicons/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'NRC-Sentiment-Emotion-Lexicons/'\n",
    "dirs = [path+i+'/' for i in os.listdir(path) if 'NRC' in i and 'Colour' not in i]\n",
    "files = []\n",
    "for directory in dirs:\n",
    "    files +=[directory+i for i in os.listdir(directory) if not 'readme' in i.lower() and '.pdf' not in i and not 'ForVariousLanguages' in i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! wget http://sentiment.nrc.ca/lexicons-for-research/NRC-VAD-Lexicon.zip\n",
    "#! wget http://sentiment.nrc.ca/lexicons-for-research/NRC-Affect-Intensity-Lexicon.zip\n",
    "#! unzip NRC-Affect-Intensity-Lexicon.zip\n",
    "#! unzip NRC-VAD-Lexicon.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NRC-Sentiment-Emotion-Lexicons/NRC-Affect-Intensity-Lexicon/NRC-AffectIntensity-Lexicon.txt Index(['term', 'score', 'AffectDimension'], dtype='object')\n",
      "NRC-Sentiment-Emotion-Lexicons/NRC-Emotion-Lexicon-v0.92/NRC-Emotion-Lexicon-Senselevel-v0.92.txt Index(['gut--opening, fistula, tubule', 'fear', '0'], dtype='object')\n",
      "NRC-Sentiment-Emotion-Lexicons/NRC-Emotion-Lexicon-v0.92/NRC-Emotion-Lexicon-Wordlevel-v0.92.txt Index(['aback', 'anger', '0'], dtype='object')\n",
      "NRC-Sentiment-Emotion-Lexicons/NRC-VAD-Lexicon/NRC-VAD-Lexicon.txt Index(['Word', 'Valence', 'Arousal', 'Dominance'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "dfs = []\n",
    "for filename in files:\n",
    "    if 'Older' in filename:\n",
    "        continue\n",
    "    if not '.txt' in filename:\n",
    "        continue\n",
    "    try:\n",
    "        df = pd.read_csv(filename,sep='\\t')\n",
    "        print(filename,df.columns)\n",
    "    except:\n",
    "        print(filename)\n",
    "    dfs.append(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "AIL_df = dfs[0]\n",
    "VAD_df = dfs[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 637,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'arousal': 0.6605000000000001, 'dominance': 0.5515, 'valence': 0.5155}"
      ]
     },
     "execution_count": 637,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "w2scores = {}\n",
    "for w,val,ar,dom in VAD_df.values:\n",
    "    w2scores[w] = {'valence':val,'arousal':ar,'dominance':dom}\n",
    "def get_vad_score(doc,tokenizer=nltk.word_tokenize,agg='mean'):\n",
    "    if type(doc)==str:\n",
    "        doc = tokenizer(doc)\n",
    "    assert type(doc)==list,\"please input either a list or a string\"\n",
    "        \n",
    "    matches = []\n",
    "    for w in doc:\n",
    "        w = w.lower()\n",
    "        if w in w2scores:\n",
    "            matches.append(w2scores[w])\n",
    "    if len(matches)==0:\n",
    "        return {'arousal':np.nan,'dominance':np.nan,'valence':np.nan}\n",
    "    scores = pd.DataFrame(matches)\n",
    "    if agg=='mean':\n",
    "        scores =  scores.mean()\n",
    "    elif agg=='max':\n",
    "        scores =  scores.max()\n",
    "    else:\n",
    "        scores =  agg(scores)\n",
    "    return dict(scores)\n",
    "pickle.dump(w2scores,open('lexicon_functions/vad_score.pkl','wb'))\n",
    "w2scores = pickle.load(open('lexicon_functions/vad_score.pkl','rb'))\n",
    "get_vad_score('and I love you! hate you',agg='mean')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>term</th>\n",
       "      <th>score</th>\n",
       "      <th>AffectDimension</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1417</th>\n",
       "      <td>feeling</td>\n",
       "      <td>0.147</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2838</th>\n",
       "      <td>feeling</td>\n",
       "      <td>0.328</td>\n",
       "      <td>fear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4458</th>\n",
       "      <td>feeling</td>\n",
       "      <td>0.172</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5440</th>\n",
       "      <td>feeling</td>\n",
       "      <td>0.359</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         term  score AffectDimension\n",
       "1417  feeling  0.147           anger\n",
       "2838  feeling  0.328            fear\n",
       "4458  feeling  0.172         sadness\n",
       "5440  feeling  0.359             joy"
      ]
     },
     "execution_count": 344,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#AIL_df.columns\n",
    "AIL_df[AIL_df.term=='feeling']#.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 638,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'anger': 0.414, 'fear': 0.242, 'joy': 0.414, 'sadness': 0.32799999999999996}"
      ]
     },
     "execution_count": 638,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "w2affect = []\n",
    "for w,score,dim in AIL_df.values:\n",
    "    w2affect.append({dim:score,'w':w})\n",
    "w2affects = {}\n",
    "df = pd.DataFrame(w2affect)\n",
    "df = df.groupby('w').sum().reset_index()\n",
    "for w,anger,fear,joy,sadness in df[['w','anger','fear','joy','sadness']].values:\n",
    "    w2affects[w] = {'anger':anger,'joy':joy,'sadness':sadness,'fear':fear}\n",
    "\n",
    "def get_affect_intensity_score(doc,tokenizer=nltk.word_tokenize,agg='mean'):\n",
    "    if type(doc)==str:\n",
    "        doc = tokenizer(doc)\n",
    "    assert type(doc)==list,\"please input either a list or a string\"\n",
    "    matches = []\n",
    "    for w in doc:\n",
    "        w = w.lower()\n",
    "        if w in w2affects:\n",
    "            matches.append(w2affects[w])\n",
    "    if len(matches)==0:\n",
    "        return {'anger':np.nan,'joy':np.nan,'sadness':np.nan,'fear':np.nan}\n",
    "    scores = pd.DataFrame(matches)\n",
    "    if agg=='mean':\n",
    "        scores = scores.mean()\n",
    "    elif agg=='max':\n",
    "        scores =  scores.max()\n",
    "    else:\n",
    "        scores = agg(scores)\n",
    "    return dict(scores)\n",
    "pickle.dump(w2affects,open('lexicon_functions/ail_score.pkl','wb'))\n",
    "w2affects = pickle.load(open('lexicon_functions/ail_score.pkl','rb'))\n",
    "get_affect_intensity_score('hello I love you and you suck so much balls. I hate you. ')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Afinn\n",
    "Finn Nielsen (DTU)\n",
    "\n",
    "http://www2.imm.dtu.dk/pubdb/views/publication_details.php?id=6010"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "! pip install afinn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 580,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'afinn': 3.0}"
      ]
     },
     "execution_count": 580,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from afinn import Afinn\n",
    "afinn = Afinn(emoticons=True)\n",
    "def get_afinn(text):\n",
    "    if type(text)==list:\n",
    "        text = ' '.join(text)\n",
    "    return {'afinn':afinn.score(text)}\n",
    "#get_afinn('hello I love you so much')\n",
    "get_afinn(['hello','I','love','you','so','much'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conglomerate\n",
    "- Bing, Liu Opinion\n",
    "- MPQA subjectivity\n",
    "- Harvard General Inquirer\n",
    "- NRC Emotion\n",
    "\t\t\t\t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 586,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('https://raw.githubusercontent.com/beefoo/text-analysis/master/lexicons/lexicons_compiled.csv')\n",
    "df_dummy = pd.get_dummies(df[['emotion','orientation','sentiment','subjectivity']])\n",
    "idx = (df.apply(lambda x: x.apply(lambda x: type(x)==str),axis=1)).sum(axis=1).sort_values(ascending=False).index\n",
    "w2conglomerate = dict(list(zip(df.word,df_dummy.values)))\n",
    "conglomerate_cols = df_dummy.columns\n",
    "\n",
    "\n",
    "\n",
    "def get_conglomerate_scores(doc,tokenizer=nltk.word_tokenize,agg='mean'):\n",
    "    if type(doc)==str:\n",
    "        doc = tokenizer(doc)\n",
    "    assert type(doc)==list,\"please input either a list or a string\"\n",
    "    matches = []\n",
    "    for w in doc:\n",
    "        w = w.lower()\n",
    "        if w in w2conglomerate:\n",
    "            matches.append(dict(list(zip(conglomerate_cols,w2conglomerate[w]))))\n",
    "    if len(matches)==0:\n",
    "        return dict(list(zip(conglomerate_cols,[np.nan]*len(conglomerate_cols))))\n",
    "    scores = pd.DataFrame(matches)\n",
    "    if agg=='mean':\n",
    "        scores = scores.mean()\n",
    "    elif agg =='max':\n",
    "        scores  = scores.max()\n",
    "    else:\n",
    "        assert hasattr(agg,'__call__'),'\"agg\" should be a function if not \"mean\" or \"max\"'\n",
    "        scores = agg(scores)\n",
    "    return dict(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 639,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump([w2conglomerate,conglomerate_cols],open('lexicon_functions/conglomerate.pkl','wb'))\n",
    "#w2conglomerate,conglomerate_cols,get_conglomerate_scores = pickle.load(open('lexicon_functions/conglomerate.pkl','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 593,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'emotion_anger': 0.0,\n",
       " 'emotion_anticipation': 0.0,\n",
       " 'emotion_disgust': 0.0,\n",
       " 'emotion_fear': 0.0,\n",
       " 'emotion_joy': 0.0,\n",
       " 'emotion_sadness': 0.0,\n",
       " 'emotion_surprise': 0.0,\n",
       " 'emotion_trust': 0.0,\n",
       " 'orientation_active': 0.5,\n",
       " 'orientation_passive': 0.3333333333333333,\n",
       " 'sentiment_negative': 0.0,\n",
       " 'sentiment_positive': 0.3333333333333333,\n",
       " 'subjectivity_strong': 0.3333333333333333,\n",
       " 'subjectivity_weak': 0.16666666666666666}"
      ]
     },
     "execution_count": 593,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2conglomerate,conglomerate_cols = pickle.load(open('lexicon_functions/conglomerate.pkl','rb'))\n",
    "get_conglomerate_scores('Do you like drugs? uugs. Do you like druuggs? I do. So what')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Positive and negative Liu\n",
    "https://www.cs.uic.edu/~liub/FBS/sentiment-analysis.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 644,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'positive_count': 2, 'negative_count': 1}"
      ]
     },
     "execution_count": 644,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import opinion_lexicon\n",
    "positive_w = set(opinion_lexicon.positive())\n",
    "negative_w = set(opinion_lexicon.negative())\n",
    "\n",
    "\n",
    "def get_pos_neg_liu(doc,tokenizer=nltk.word_tokenize,agg='sum'):\n",
    "    if type(doc)==str:\n",
    "        doc = [i.lower() for i in tokenizer(doc)]\n",
    "    assert type(doc)==list, 'input has to be either string or list'\n",
    "    if len(doc)==0:\n",
    "        return {'positive_count':np.nan,'negative_count':np.nan}\n",
    "    \n",
    "    d = {'positive_count':count_words(doc,positive_w),\n",
    "        'negative_count':count_words(doc,negative_w)}\n",
    "    if agg=='sum':\n",
    "        return d\n",
    "    elif agg=='mean':\n",
    "        return {key:val/len(doc) for key,val in d.items()}\n",
    "    \n",
    "def count_words(doc,s):\n",
    "    c = Counter(doc)\n",
    "    return sum([c[i] for i in s])\n",
    "get_pos_neg_liu('you love hating me right?')    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hedometer\n",
    "https://hedonometer.org/index.html\n",
    "\n",
    "https://raw.githubusercontent.com/andyreagan/hedonometer/master/hedonometer/static/hedonometer/labMT1.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 642,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('https://raw.githubusercontent.com/andyreagan/hedonometer/master/hedonometer/static/hedonometer/labMT1.txt',sep='\\t',\n",
    "                header =None,names = ['w','rank','score','std','twitter','googlebooks','newyorktimes','lyrics'])[['w','score','std']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 643,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'happiness': 22.54}"
      ]
     },
     "execution_count": 643,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2happy = dict(df[['w','score']].values)\n",
    "def get_happiness(doc,tokenizer=nltk.word_tokenize,agg='sum'):\n",
    "    if type(doc)==str:\n",
    "        doc = tokenizer(doc)\n",
    "    assert type(doc)==list, 'please input string or list'\n",
    "\n",
    "    scores = []\n",
    "    for w in doc:\n",
    "        if w in w2happy:\n",
    "            scores.append(w2happy[w])\n",
    "    score = np.mean(scores)\n",
    "    return {'happiness':score}\n",
    "pickle.dump(w2happy,open('lexicon_functions/happiness.pkl','wb'))\n",
    "w2happy = pickle.load(open('lexicon_functions/happiness.pkl','rb'))\n",
    "get_happiness('i love laughing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 604,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10221, 8)"
      ]
     },
     "execution_count": 604,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrap them all in one big function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w2scores,get_subjectivity = pickle.load(open('lexicon_functions/subjectivity_score.pkl','rb'))\n",
    "text2argfeatures,class2re,string_test = pickle.load(open('lexicon_functions/text2arg.pkl','rb'))\n",
    "w2scores,get_vad_score = pickle.load(open('lexicon_functions/vad_score.pkl','rb'))\n",
    "w2affects,get_affect_intensity_score = pickle.load(open('lexicon_functions/ail_score.pkl','rb'))\n",
    "w2conglomerate,conglomerate_cols,get_conglomerate_scores = pickle.load(open('lexicon_functions/conglomerate.pkl','rb'))\n",
    "w2happy,get_happiness = pickle.load(open('lexicon_functions/happiness.pkl','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 631,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "vader_neg                               0.102000\n",
       "vader_neu                               0.783000\n",
       "vader_pos                               0.115000\n",
       "vader_compound                          0.985500\n",
       "afinn_afinn                            65.000000\n",
       "argumentation_inconsistency            18.000000\n",
       "argumentation_conditionals              8.000000\n",
       "argumentation_contrast                 12.000000\n",
       "argumentation_emphasis                 30.000000\n",
       "argumentation_causation                38.000000\n",
       "argumentation_wants                     6.000000\n",
       "argumentation_difficulty               11.000000\n",
       "argumentation_inyourshoes               4.000000\n",
       "argumentation_rhetoricalquestion        5.000000\n",
       "argumentation_assessments              24.000000\n",
       "argumentation_generalization            5.000000\n",
       "argumentation_structure                 3.000000\n",
       "argumentation_necessity                25.000000\n",
       "argumentation_doubt                     4.000000\n",
       "argumentation_priority                  8.000000\n",
       "argumentation_possibility              21.000000\n",
       "argumentation_authority                 1.000000\n",
       "liu_positive_count                     58.000000\n",
       "liu_negative_count                     57.000000\n",
       "conglomerate_emotion_anger              0.032787\n",
       "conglomerate_emotion_anticipation       0.050820\n",
       "conglomerate_emotion_disgust            0.003279\n",
       "conglomerate_emotion_fear               0.032787\n",
       "conglomerate_emotion_joy                0.021311\n",
       "conglomerate_emotion_sadness            0.016393\n",
       "conglomerate_emotion_surprise           0.003279\n",
       "conglomerate_emotion_trust              0.040984\n",
       "conglomerate_orientation_active         0.308197\n",
       "conglomerate_orientation_passive        0.147541\n",
       "conglomerate_sentiment_negative         0.175410\n",
       "conglomerate_sentiment_positive         0.337705\n",
       "conglomerate_subjectivity_strong        0.244262\n",
       "conglomerate_subjectivity_weak          0.285246\n",
       "affect_intensity_anger                  0.094366\n",
       "affect_intensity_fear                   0.142455\n",
       "affect_intensity_joy                    0.221614\n",
       "affect_intensity_sadness                0.154911\n",
       "vad_arousal                             0.440817\n",
       "vad_dominance                           0.536227\n",
       "vad_valence                             0.611001\n",
       "subjectivity_weaksubj                   0.081405\n",
       "subjectivity_strongsubj                 0.056984\n",
       "hedometer_happiness                  8876.650000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 631,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name2func = {'liu':get_pos_neg_liu,\n",
    "             'conglomerate':get_conglomerate_scores,\n",
    "             'affect_intensity':get_affect_intensity_score,\n",
    "             'vad':get_vad_score,\n",
    "             'subjectivity':get_subjectivity,\n",
    "             'hedometer':get_happiness\n",
    "            }\n",
    "textbased_funcs = {'vader':vader.polarity_scores,\n",
    "             'afinn':get_afinn,\n",
    "                  'argumentation':text2argfeatures}\n",
    "\n",
    "def lexical_mining(text,tokenizer = nltk.word_tokenize,agg = {}):\n",
    "    if type(text)==str:\n",
    "        doc = tokenizer(text)\n",
    "    if type(text)==np.nan:\n",
    "        return np.nan\n",
    "    d = {}\n",
    "    for name,func in textbased_funcs.items():\n",
    "        temp_d = {'%s_%s'%(name,key):val for key,val in func(text).items()}\n",
    "        d.update(temp_d)\n",
    "    for name,func in name2func.items():\n",
    "        temp_d = {'%s_%s'%(name,key):val for key,val in func(doc).items()}\n",
    "        d.update(temp_d)\n",
    "    return pd.Series(d)\n",
    "        \n",
    "lexical_mining(string_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiwordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import sentiwordnet as swn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SentiSynset('decelerate.v.01'),\n",
       " SentiSynset('slow.v.02'),\n",
       " SentiSynset('slow.v.03'),\n",
       " SentiSynset('slow.a.01'),\n",
       " SentiSynset('slow.a.02'),\n",
       " SentiSynset('dense.s.04'),\n",
       " SentiSynset('slow.a.04'),\n",
       " SentiSynset('boring.s.01'),\n",
       " SentiSynset('dull.s.08'),\n",
       " SentiSynset('slowly.r.01'),\n",
       " SentiSynset('behind.r.03')]"
      ]
     },
     "execution_count": 462,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(swn.senti_synsets('slow'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SentiSynset('happy.a.01'),\n",
       " SentiSynset('felicitous.s.02'),\n",
       " SentiSynset('glad.s.02'),\n",
       " SentiSynset('happy.s.04')]"
      ]
     },
     "execution_count": 464,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "happy = swn.senti_synsets('happy', 'a')\n",
    "list(happy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
