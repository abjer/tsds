{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DO NOT EDIT THIS FILE WITHIN THE /TSDS FOLDER - YOU RISK OVERWRITING YOUR WORK THE NEXT TIME YOU PULL FROM THE GITHUB REPOSITORY**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 4\n",
    "\n",
    "\n",
    "### Practical info\n",
    "* Handin in absalon. The deadline is the 16th of May (see the [course plan](https://github.com/abjer/tsds/wiki/Course-plan))\n",
    "* You must work in groups of 2-4. **Remember to identify the group members in the filename or in the top of the file contents**.\n",
    "* If anything is unclear dont hesitate to email me at kuol@econ.ku.dk with questions.\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "# Questions from exercise set 11 (Text #1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import codecs\n",
    "import requests\n",
    "import wordcloud\n",
    "import numpy as np \n",
    "from collections import Counter\n",
    "import itertools\n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "\n",
    "# This pulls Snorre's expore_regex script from github \n",
    "# useful to set up an explorer that can be used to iteratively\n",
    "# find good regular expressions\n",
    "#\n",
    "# use the module to develop the pattern and finally use the .report() method to document your process. \n",
    "\n",
    "with open('explore_regex.py','w') as f:\n",
    "    f.write(requests.get('https://raw.githubusercontent.com/snorreralund/explore_regex/master/explore_regex.py').text)\n",
    "    \n",
    "import explore_regex\n",
    "\n",
    "\n",
    "# LOAD DATA (available on absalon)\n",
    "# Load overflow data\n",
    "overflow_df = pd.read_csv('Posts_overflow.csv').dropna(subset = ['Body'])\n",
    "# Load the datascience data. \n",
    "datascience_df = pd.read_csv('Posts_ds.csv').dropna(subset = ['Body'])\n",
    "# load stats data\n",
    "stats_df = pd.read_csv('Posts_stats.csv').dropna(subset = ['Body'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ex.11.1.1** _Cleaning_\n",
    "First task is as with any other data experience (especially text data): It needs Cleaning. Here we need to brush up on our string and scraping fundamentals: Regular expressions and BeautifulSoup.\n",
    "\n",
    "Before we apply BeatifulSoup to get the clean text from HTML. We need to do some \"pre-preprocessing\".\n",
    "We shall take advantage of the HTML tags to extract code from text.\n",
    "\n",
    "Extract the code segments using the HTML tag: \"<\\CODE>\", and put it into a separate columns for later analysis.\n",
    "\n",
    "- Design a regular expression that match anything inside a `<code></code>` html-tag. \n",
    "> _Hint:_ inspect some of the data to see exactly how the code is wrapped in the html.\n",
    "- Use it to extract it to another column.\n",
    "- Use it to remove the code from the Body column.\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Answer to ex. 11.1.1 here]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ex.11.1.4** Finally Extract the individual tags from the Tags columns using regular expressions. \n",
    "- Design a regular expression that matches the characters and symbols within the <> brackets. <[symbolsgoeshere]>, and assign it to a column named `tags_l` that holds a list of tags. \n",
    "- Count the Tags and Visualize them using the WordCloud package (https://github.com/amueller/word_cloud ). See this simple example: https://github.com/amueller/word_cloud/blob/master/examples/simple.py \n",
    "> *Hint:* use the method: .generate_from_frequencies(). That takes a dictionary of strings and their counts as input.\n",
    "\n",
    "**Extra:** \n",
    "Visualize two tag sets:\n",
    "    - one that co-occur with the <r> tag\n",
    "    - and another for the <python>.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_from_to_column(df, regex, from_col, to_col):\n",
    "    df[to_col] = df[from_col].apply(lambda x: ' '.join(regex.findall(x)))    \n",
    "    return df\n",
    "\n",
    "comment_str = '#.*$'\n",
    "comment_regex = re.compile(comment_str, flags = re.DOTALL|re.UNICODE)\n",
    "\n",
    "overflow_df = extract_from_to_column(overflow_df, comment_regex, from_col = 'code', to_col = 'comments')\n",
    "datascience_df = extract_from_to_column(datascience_df, comment_regex, from_col = 'code', to_col = 'comments')\n",
    "stats_df = extract_from_to_column(stats_df, comment_regex, from_col = 'code', to_col = 'comments')\n",
    "\n",
    "\n",
    "def get_text(html):\n",
    "    soup = BeautifulSoup(html, 'lxml')\n",
    "    text = soup.get_text()\n",
    "    return text\n",
    "\n",
    "def extract_text(df):    \n",
    "    df['text'] = df.Body.apply(lambda x: get_text(x))    \n",
    "    return df \n",
    "\n",
    "overflow_df    = extract_text(overflow_df)\n",
    "datascience_df = extract_text(datascience_df)\n",
    "stats_df       = extract_text(stats_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Answer to ex. 11.1.4 here]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "# Questions from exercise set 12 (Text #2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import pickle \n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "\n",
    "# LOAD DATA\n",
    "# Load overflow data\n",
    "overflow_df = pd.read_csv('Posts_overflow.csv').dropna(subset = ['Body'])\n",
    "# Load the datascience data. \n",
    "datascience_df = pd.read_csv('Posts_ds.csv').dropna(subset = ['Body'])\n",
    "# load stats data\n",
    "stats_df = pd.read_csv('Posts_stats.csv').dropna(subset = ['Body'])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def extract_from_to_column(df, regex, from_col, to_col):\n",
    "    df[to_col] = df[from_col].apply(lambda x: ' '.join(regex.findall(x)))    \n",
    "    return df\n",
    "\n",
    "code_str = '<pre><code>.+?</code></pre>'\n",
    "code_regex = re.compile(code_str, flags = re.DOTALL|re.UNICODE)\n",
    "\n",
    "\n",
    "# Then we remove the code from the html body\n",
    "def remove_code_from_body(df, regex):\n",
    "    df['Body'] = df.Body.apply(lambda x: re.sub(regex, ' ', x))\n",
    "    return df \n",
    "\n",
    "\n",
    "comment_str = '#.*$'\n",
    "comment_regex = re.compile(comment_str, flags = re.DOTALL|re.UNICODE)\n",
    "\n",
    "def get_text(html):\n",
    "    soup = BeautifulSoup(html, 'lxml')\n",
    "    text = soup.get_text()\n",
    "    return text\n",
    "\n",
    "def extract_text(df):    \n",
    "    df['text'] = df.Body.apply(lambda x: get_text(x))    \n",
    "    return df \n",
    "\n",
    "\n",
    "tag_str = '(?<=<).*?(?=>)'\n",
    "tag_re = re.compile(tag_str, flags = re.DOTALL|re.UNICODE)\n",
    "\n",
    "\n",
    "def tags_if_any(x, regex):\n",
    "    if pd.isna(x):\n",
    "        return list()\n",
    "    return regex.findall(x)\n",
    "\n",
    "def find_tags(df, regex):\n",
    "    df['tags_l'] = df.Tags.apply(lambda x: tags_if_any(x, regex))\n",
    "    return df \n",
    "\n",
    "\n",
    "\n",
    "overflow_df    = extract_from_to_column(overflow_df, code_regex, from_col = 'Body', to_col = 'code')\n",
    "datascience_df = extract_from_to_column(datascience_df, code_regex, from_col = 'Body', to_col = 'code')\n",
    "stats_df       = extract_from_to_column(stats_df, code_regex, from_col = 'Body', to_col = 'code')\n",
    "\n",
    "overflow_df    = remove_code_from_body(overflow_df, code_regex)\n",
    "datascience_df = remove_code_from_body(datascience_df, code_regex)\n",
    "stats_df       = remove_code_from_body(stats_df, code_regex)\n",
    "\n",
    "overflow_df = extract_from_to_column(overflow_df, comment_regex, from_col = 'code', to_col = 'comments')\n",
    "datascience_df = extract_from_to_column(datascience_df, comment_regex, from_col = 'code', to_col = 'comments')\n",
    "stats_df = extract_from_to_column(stats_df, comment_regex, from_col = 'code', to_col = 'comments')\n",
    "\n",
    "overflow_df    = extract_text(overflow_df)\n",
    "datascience_df = extract_text(datascience_df)\n",
    "stats_df       = extract_text(stats_df)\n",
    "\n",
    "overflow_df    = find_tags(overflow_df, tag_re).sample(20000)\n",
    "datascience_df = find_tags(datascience_df, tag_re).sample(20000)\n",
    "stats_df       = find_tags(stats_df, tag_re).sample(20000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 12.1.1** before we move on to prebuild lexicons we will construct our own simple example. Begin by defining two dictionaries with keys corresponding to the words you want to search for and a value of `1` for each key (we will simply count the number of occurences of our words). An example of lexica could be \n",
    "\n",
    "```python\n",
    "py_lexicon = {'python': 1,\n",
    "              'py': 1,\n",
    "              'ipy':1}\n",
    "\n",
    "r_lexicon = {'r': 1,\n",
    "             'tidyverse': 1,\n",
    "             'tidy':1}\n",
    "```\n",
    "\n",
    "Create a new column in each of the dataset which contains a list of each word in the `text` column (hint: remember to lower/upper all characters here, to match both _'Python'_ and _'python'_. Is this a good way to tokenize the text? Could you come up with improvements?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Answer to ex. 12.1.1 here]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ex 12.1.2** Count for each observation in the datasets how many times one of the words from each of the lexica occurs in the text and store the results in two new columns called `py_hits` and `r_hits`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Answer to ex. 12.1.2 here]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ex. 12.1.3** Show the joint distribution of hits in the `r` and `py` lexica by plotting them in a scatter plot. You should plot a separate scatter for each of the forums (datasets). \n",
    "\n",
    "> _Hint:_ Use the following function to give the points a slight randomness in position. This is visually appealing when looking at  discrete data, and give some insight into the density.\n",
    ">\n",
    "> ```python\n",
    "> def rand_jitter(arr):\n",
    ">    stdev = .01*(max(arr)-min(arr))\n",
    ">    return arr + np.random.randn(len(arr)) * stdev\n",
    "> ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "# Questions from exercise set 13 (Text #3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ex. 13.1.3 (BONUS)\n",
    "\n",
    "**Note: this assignment is easiest run in google colab, as deepmoji is difficult to install on windows (and some mac) computers. To get the code required to answer this question look up the solutions to the exercise set on github. As a solution you can either add screenshots or a short description of your results**\n",
    "\n",
    "- Join the output of Deepmoji with the bias dataframe columns (Race, Gender and Emotion)\n",
    "    - Make sure Race count and Gender counts are equal after join.\n",
    "\n",
    "Investigate if there are significant differences in relation to **Race** (Race column).\n",
    "\n",
    "- See which types of emojies are most different.\n",
    "\n",
    "- Make a dictionary mapping emojiis to different classes, either happy, sad, angry etc.\n",
    "- HINT: Fastests way is to loop through the list of emojiies <code>to_emoji</code> and use the <code>input()</code> to input the class.\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Answer to ex. 13.1.3 here]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
